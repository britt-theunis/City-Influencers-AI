{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Detectie van ongepaste tekst in posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Importeren van packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Eigenaar\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from nltk.corpus import movie_reviews \n",
    "from nltk.classify import NaiveBayesClassifier\n",
    "from nltk.classify.util import accuracy as nltk_accuracy\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "from nltk.corpus import stopwords \n",
    "stopwords_english = stopwords.words('english')\n",
    "import string \n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inlezen van data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_temp= pd.read_csv('data/olid-training.tsv', sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A -> offensive/ not offensive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "offensive = []\n",
    "notOffensive = []\n",
    "training_a = training_temp[['tweet', 'subtask_a']]\n",
    "test_a_values = pd.read_csv('data/testset-levela.tsv', sep='\\t')\n",
    "test_a_labels = pd.read_csv('data/labels-levela.csv')\n",
    "test_a = test_a_values.merge(test_a_labels, on=\"id\")\n",
    "\n",
    "for index, row in training_a.iterrows():\n",
    "    if(row['subtask_a']) == 'OFF':\n",
    "        offensive.append(row['tweet'])\n",
    "    else:\n",
    "        notOffensive.append(row['tweet'])\n",
    "\n",
    "for index, row in test_a.iterrows():\n",
    "    if(row['subtask_a']) == 'OFF':\n",
    "        offensive.append(row['tweet'])\n",
    "    else:\n",
    "        notOffensive.append(row['tweet'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B -> targeted/ untargeted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "targeted = []\n",
    "notTargeted = []\n",
    "training_b = training_temp[['tweet', 'subtask_b']]\n",
    "test_b_values = pd.read_csv('data/testset-levelb.tsv', sep='\\t')\n",
    "test_b_labels = pd.read_csv('data/labels-levelb.csv')\n",
    "test_b = test_b_values.merge(test_b_labels, on=\"id\")\n",
    "\n",
    "for index, row in training_b.iterrows():\n",
    "    if(row['subtask_b']) == 'TIN':\n",
    "        targeted.append(row['tweet'])\n",
    "    else:\n",
    "        notTargeted.append(row['tweet'])\n",
    "\n",
    "for index, row in test_b.iterrows():\n",
    "    if(row['subtask_b']) == 'TIN':\n",
    "        targeted.append(row['tweet'])\n",
    "    else:\n",
    "        notTargeted.append(row['tweet'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C -> offense target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "individual = []\n",
    "group = []\n",
    "other = []\n",
    "training_c = training_temp[['tweet', 'subtask_c']]\n",
    "test_c_values = pd.read_csv('data/testset-levelc.tsv', sep='\\t')\n",
    "test_c_labels = pd.read_csv('data/labels-levelc.csv')\n",
    "test_c = test_c_values.merge(test_c_labels, on=\"id\")\n",
    "\n",
    "for index, row in training_c.iterrows():\n",
    "    if(row['subtask_c']) == 'IND':\n",
    "        individual.append(row['tweet'])\n",
    "    elif(row['subtask_c']) ==  'GRP':\n",
    "        group.append(row['tweet'])\n",
    "    else:\n",
    "        other.append(row['tweet'])\n",
    "\n",
    "for index, row in test_c.iterrows():\n",
    "    if(row['subtask_c']) == 'IND':\n",
    "        individual.append(row['tweet'])\n",
    "    elif(row['subtask_c']) ==  'GRP':\n",
    "        group.append(row['tweet'])\n",
    "    else:\n",
    "        other.append(row['tweet'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Detecteren van ongepast taalgebruik"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A -> offensive/ not offensive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Een list creëren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "offensive_list_clean = []\n",
    "notOffensive_list_clean = []\n",
    "words_clean_a = []\n",
    "\n",
    "for o in offensive:\n",
    "    offensive_temp = []\n",
    "    for word in word_tokenize(o):\n",
    "        word = word.lower()\n",
    "        if word not in stopwords_english and word not in string.punctuation and len(word) > 1 and not word.isdigit():\n",
    "            offensive_temp.append(word)\n",
    "            words_clean_a.append(word)\n",
    "    offensive_list_clean.append(offensive_temp)\n",
    "\n",
    "for n in notOffensive:\n",
    "    notOffensive_temp = []\n",
    "    for word in word_tokenize(n):\n",
    "        word = word.lower()\n",
    "        if word not in stopwords_english and word not in string.punctuation and len(word) > 1 and not word.isdigit():\n",
    "            notOffensive_temp.append(word)\n",
    "            words_clean_a.append(word)\n",
    "    notOffensive_list_clean.append(notOffensive_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_a = []\n",
    "\n",
    "for o in offensive_list_clean:\n",
    "    all_a.append((o,'offensive'))\n",
    "\n",
    "for n in notOffensive_list_clean:\n",
    "    all_a.append((n,'not offensive'))\n",
    "\n",
    "random.shuffle(all_a)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### De top 3000 woorden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('user', 34024), ('url', 2602), (\"''\", 2219), (\"'s\", 1531), ('liberals', 1485), ('gun', 1427), (\"n't\", 1403), ('...', 1304), ('control', 1286), ('antifa', 1244), ('like', 1172), ('maga', 1067), ('conservatives', 1029), ('people', 968), ('get', 713), ('one', 688), ('trump', 682), ('amp', 682), ('know', 669), ('would', 581)]\n"
     ]
    }
   ],
   "source": [
    "word_frequency_a = nltk.FreqDist(words_clean_a)\n",
    "print(word_frequency_a.most_common(20))\n",
    "top_words_a = list(word_frequency_a.keys())[:3000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### De featureset creëren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_top_words(words):\n",
    "    wordset = set(words)\n",
    "    result = {}\n",
    "    for w in top_words_a:\n",
    "        result[w] = (w in wordset) # true if top_word is occurring in the wordset\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets_a = []\n",
    "for (words, category) in all_a:\n",
    "    featuresets_a.append((find_top_words(words), category))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### De classifier trainen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy of the NaiveBayesClassifier: 0.7528966658784583\n"
     ]
    }
   ],
   "source": [
    "index = int(len(featuresets_a)*0.7+1)\n",
    "training_set_a = featuresets_a[:index]\n",
    "test_set_a = featuresets_a[index:]\n",
    "\n",
    "classifier_a = nltk.NaiveBayesClassifier.train(training_set_a)\n",
    "print('\\nAccuracy of the NaiveBayesClassifier:', nltk_accuracy(classifier_a, test_set_a))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                   bitch = True           offens : not of =     57.4 : 1.0\n",
      "                   idiot = True           offens : not of =     56.4 : 1.0\n",
      "                  idiots = True           offens : not of =     27.8 : 1.0\n",
      "                    fuck = True           offens : not of =     23.8 : 1.0\n",
      "                  stupid = True           offens : not of =     19.1 : 1.0\n",
      "                 asshole = True           offens : not of =     18.3 : 1.0\n",
      "                  coward = True           offens : not of =     18.3 : 1.0\n",
      "                  fucked = True           offens : not of =     17.5 : 1.0\n",
      "                    shit = True           offens : not of =     17.2 : 1.0\n",
      "                    ugly = True           offens : not of =     16.7 : 1.0\n",
      "                   pussy = True           offens : not of =     15.1 : 1.0\n",
      "                 bitches = True           offens : not of =     14.3 : 1.0\n",
      "                 fucking = True           offens : not of =     13.8 : 1.0\n",
      "                ignorant = True           offens : not of =     13.1 : 1.0\n",
      "                  morons = True           offens : not of =     12.9 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# de waardevolste woorden\n",
    "classifier_a.show_most_informative_features(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B -> targeted/ untargeted"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Een list creëren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "targeted_list_clean = []\n",
    "notTargeted_list_clean = []\n",
    "words_clean_b = []\n",
    "\n",
    "for t in targeted:\n",
    "    targeted_temp = []\n",
    "    for word in word_tokenize(t):\n",
    "        word = word.lower()\n",
    "        if word not in stopwords_english and word not in string.punctuation and len(word) > 1 and not word.isdigit():\n",
    "            targeted_temp.append(word)\n",
    "            words_clean_b.append(word)\n",
    "    targeted_list_clean.append(targeted_temp)\n",
    "\n",
    "for nt in notTargeted:\n",
    "    notTargeted_temp = []\n",
    "    for word in word_tokenize(nt):\n",
    "        word = word.lower()\n",
    "        if word not in stopwords_english and word not in string.punctuation and len(word) > 1 and not word.isdigit():\n",
    "            notTargeted_temp.append(word)\n",
    "            words_clean_b.append(word)\n",
    "    notTargeted_list_clean.append(notTargeted_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_b = []\n",
    "\n",
    "for t in targeted_list_clean:\n",
    "    all_b.append((t,'targeted'))\n",
    "\n",
    "for nt in notTargeted_list_clean:\n",
    "    all_b.append((nt,'untargeted'))\n",
    "\n",
    "random.shuffle(all_b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### De top 3000 woorden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('user', 33561), ('url', 2173), (\"''\", 2155), ('liberals', 1446), (\"'s\", 1437), ('gun', 1372), (\"n't\", 1342), ('...', 1242), ('control', 1232), ('antifa', 1194), ('like', 1127), ('maga', 1023), ('conservatives', 972), ('people', 933), ('get', 686), ('amp', 677), ('one', 655), ('trump', 655), ('know', 643), ('would', 572)]\n"
     ]
    }
   ],
   "source": [
    "word_frequency_b = nltk.FreqDist(words_clean_b)\n",
    "print(word_frequency_b.most_common(20))\n",
    "top_words_b = list(word_frequency_b.keys())[:3000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### De featureset creëren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_top_words(words):\n",
    "    wordset = set(words)\n",
    "    result = {}\n",
    "    for w in top_words_b:\n",
    "        result[w] = (w in wordset) # true if top_word is occurring in the wordset\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets_b = []\n",
    "for (words, category) in all_b:\n",
    "    featuresets_b.append((find_top_words(words), category))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### De classiefier trainen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy of the NaiveBayesClassifier: 0.7378184516448182\n"
     ]
    }
   ],
   "source": [
    "index = int(len(featuresets_b)*0.7+1)\n",
    "training_set_b = featuresets_b[:index]\n",
    "test_set_b = featuresets_b[index:]\n",
    "\n",
    "classifier_b = nltk.NaiveBayesClassifier.train(training_set_b)\n",
    "print('\\nAccuracy of the NaiveBayesClassifier:', nltk_accuracy(classifier_b, test_set_b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                  coward = True           target : untarg =     23.5 : 1.0\n",
      "                   bitch = True           target : untarg =     21.7 : 1.0\n",
      "                  idiots = True           target : untarg =     18.7 : 1.0\n",
      "                 asshole = True           target : untarg =     15.0 : 1.0\n",
      "                  stupid = True           target : untarg =     15.0 : 1.0\n",
      "                  rapist = True           target : untarg =     14.4 : 1.0\n",
      "                assholes = True           target : untarg =     11.4 : 1.0\n",
      "                 bitches = True           target : untarg =     11.4 : 1.0\n",
      "               hypocrite = True           target : untarg =     11.4 : 1.0\n",
      "                traitors = True           target : untarg =     11.4 : 1.0\n",
      "                 fascist = True           target : untarg =     11.0 : 1.0\n",
      "                   idiot = True           target : untarg =     10.5 : 1.0\n",
      "              hypocrites = True           target : untarg =     10.5 : 1.0\n",
      "                   silly = True           target : untarg =     10.5 : 1.0\n",
      "                 traitor = True           target : untarg =     10.5 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# de waardevolste woorden\n",
    "classifier_b.show_most_informative_features(15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### C -> offense target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "individual_list_clean = []\n",
    "group_list_clean = []\n",
    "other_list_clean = []\n",
    "words_clean_c = []\n",
    "\n",
    "for i in individual:\n",
    "    individual_temp = []\n",
    "    for word in word_tokenize(i):\n",
    "        word = word.lower()\n",
    "        if word not in stopwords_english and word not in string.punctuation and len(word) > 1 and not word.isdigit():\n",
    "            individual_temp.append(word)\n",
    "            words_clean_c.append(word)\n",
    "    individual_list_clean.append(individual_temp)\n",
    "\n",
    "for g in group:\n",
    "    group_temp = []\n",
    "    for word in word_tokenize(g):\n",
    "        word = word.lower()\n",
    "        if word not in stopwords_english and word not in string.punctuation and len(word) > 1 and not word.isdigit():\n",
    "            group_temp.append(word)\n",
    "            words_clean_c.append(word)\n",
    "    group_list_clean.append(group_temp)\n",
    "\n",
    "for o in other:\n",
    "    other_temp = []\n",
    "    for word in word_tokenize(o):\n",
    "        word = word.lower()\n",
    "        if word not in stopwords_english and word not in string.punctuation and len(word) > 1 and not word.isdigit():\n",
    "            other_temp.append(word)\n",
    "            words_clean_c.append(word)\n",
    "    other_list_clean.append(other_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_c = []\n",
    "\n",
    "for i in individual_list_clean:\n",
    "    all_c.append((i,'individual'))\n",
    "\n",
    "for g in group_list_clean:\n",
    "    all_c.append((g,'group'))\n",
    "\n",
    "for o in other_list_clean:\n",
    "    all_c.append((o,'other'))\n",
    "\n",
    "random.shuffle(all_c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### De top 3000 woorden"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('user', 33558), ('url', 2162), (\"''\", 2153), ('liberals', 1446), (\"'s\", 1436), ('gun', 1372), (\"n't\", 1339), ('...', 1239), ('control', 1232), ('antifa', 1193), ('like', 1123), ('maga', 1022), ('conservatives', 972), ('people', 932), ('get', 684), ('amp', 677), ('trump', 655), ('one', 655), ('know', 642), ('would', 570)]\n"
     ]
    }
   ],
   "source": [
    "word_frequency_c = nltk.FreqDist(words_clean_c)\n",
    "print(word_frequency_c.most_common(20))\n",
    "top_words_c = list(word_frequency_c.keys())[:3000]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### De featureset creëren"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_top_words(words):\n",
    "    wordset = set(words)\n",
    "    result = {}\n",
    "    for w in top_words_c:\n",
    "        result[w] = (w in wordset) # true if top_word is occurring in the wordset\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresets_c = []\n",
    "for (words, category) in all_c:\n",
    "    featuresets_c.append((find_top_words(words), category))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### De classifier trainen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Accuracy of the NaiveBayesClassifier: 0.7154894671623296\n"
     ]
    }
   ],
   "source": [
    "index = int(len(featuresets_c)*0.7+1)\n",
    "training_set_c = featuresets_c[:index]\n",
    "test_set_c = featuresets_c[index:]\n",
    "\n",
    "classifier_c = nltk.NaiveBayesClassifier.train(training_set_c)\n",
    "print('\\nAccuracy of the NaiveBayesClassifier:', nltk_accuracy(classifier_c, test_set_c))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Most Informative Features\n",
      "                  idiots = True            group : other  =     89.6 : 1.0\n",
      "                  coward = True           indivi : other  =     35.1 : 1.0\n",
      "                   hates = True            group : other  =     31.8 : 1.0\n",
      "                   idiot = True           indivi : other  =     29.1 : 1.0\n",
      "                   bitch = True           indivi : other  =     27.3 : 1.0\n",
      "                 traitor = True           indivi : other  =     24.7 : 1.0\n",
      "               hypocrite = True           indivi : other  =     22.6 : 1.0\n",
      "                 bitches = True            group : other  =     22.5 : 1.0\n",
      "                 dumbass = True            group : other  =     20.2 : 1.0\n",
      "            hypocritical = True            group : other  =     20.2 : 1.0\n",
      "                     neo = True            group : other  =     20.2 : 1.0\n",
      "                    ugly = True           indivi : other  =     19.5 : 1.0\n",
      "                 asshole = True           indivi : other  =     18.0 : 1.0\n",
      "                   moron = True           indivi : other  =     16.9 : 1.0\n",
      "                   raped = True           indivi : other  =     16.9 : 1.0\n"
     ]
    }
   ],
   "source": [
    "# de waardevolste woorden\n",
    "classifier_c.show_most_informative_features(15)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e9efe275de89270cf0d9a414158c7c6b87af0a6303f6038d1925d6aaf54a8f58"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
